{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import re, string, unicodedata\n",
    "import inflect\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mwizasimbeye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Data preprocessing, our data is in .doc and docx format. We need to convert the files to .txt to enable easy preprocessing.\n",
    "The mothod convert_resume() reads all the resumes in a provided folder. Note: The prepocessor uses a Mac tool called **textutil**. For linux users, please use **pandoc**. example: *pandoc -s example.docx -o output.txt*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our method.\n",
    "def convert_resumes(path):\n",
    "    dirs = [os.path.join(path, o) for o in os.listdir(path) if os.path.isdir(os.path.join(path,o))]\n",
    "    for x in dirs:\n",
    "        x = x.replace(\" \", \"\\ \")\n",
    "        os.system('textutil -convert txt '+ x +'/*.doc*')\n",
    "    return \"Converted files to .txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and preprocess the resume. \n",
    "def process_resume(filename):\n",
    "    # load text\n",
    "    file = open(filename, 'rt')\n",
    "    text = file.read()\n",
    "    text = ' '.join(text.split()) # Remove double spaces\n",
    "    file.close()\n",
    "    # Remove the common hex values from text.\n",
    "    text = text.replace(\"\\xe2\", \"\").replace(\"\\x80\", \"\").replace(\"\\x93\", \"\").replace(\"\\x99\", \"\").replace(\"0x9c\", \"\")\n",
    "    text = text.lower()    \n",
    "    text = ' '.join(text.split())\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(words):\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job titles & candidate name\n",
    "path = 'data/'\n",
    "col_names =  ['candidate_name', 'job_title', 'resume']\n",
    "data  = pd.DataFrame(columns = col_names)\n",
    "dirs = [os.path.join(path, o) for o in os.listdir(path) if os.path.isdir(os.path.join(path,o))]\n",
    "for x in dirs:\n",
    "    job = (x.replace('data/', ''))\n",
    "    files = [f for f in listdir(x) if isfile(join(x, f))]\n",
    "    candidates = [f for f in files if f.endswith('.txt') if f!=\"Success.txt\" if f!=\"Job Description.txt\"]\n",
    "    for x in candidates:\n",
    "        resume = process_resume(path+\"/\"+job+\"/\"+x).split()\n",
    "        resume = normalize(resume)\n",
    "        data.loc[len(data)] = [x[:-4], job, resume]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>candidate_name</th>\n",
       "      <th>job_title</th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ardiela Dramat</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeanine Adant</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Joan Nicolene Esterhuizen</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rozina Scheepers</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samantha Jennings</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[information, concerning, candidate, furnished...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sonja Krog</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Surelda Schlebusch</td>\n",
       "      <td>administration manager</td>\n",
       "      <td>[position, applied, new, business, sales, cros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gwendolene Margaret Matjan</td>\n",
       "      <td>assistant accountant</td>\n",
       "      <td>[pdf15, one, zero, obj, typecatalogpages, two,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hallesheen Leshae Moos</td>\n",
       "      <td>assistant accountant</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Luan Benjamin</td>\n",
       "      <td>assistant accountant</td>\n",
       "      <td>[confidentiality, clause, information, concern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               candidate_name               job_title  \\\n",
       "0              Ardiela Dramat  administration manager   \n",
       "1               Jeanine Adant  administration manager   \n",
       "2   Joan Nicolene Esterhuizen  administration manager   \n",
       "3            Rozina Scheepers  administration manager   \n",
       "4           Samantha Jennings  administration manager   \n",
       "5                  Sonja Krog  administration manager   \n",
       "6          Surelda Schlebusch  administration manager   \n",
       "7  Gwendolene Margaret Matjan    assistant accountant   \n",
       "8      Hallesheen Leshae Moos    assistant accountant   \n",
       "9               Luan Benjamin    assistant accountant   \n",
       "\n",
       "                                              resume  \n",
       "0  [confidentiality, clause, information, concern...  \n",
       "1  [confidentiality, clause, information, concern...  \n",
       "2  [confidentiality, clause, information, concern...  \n",
       "3  [confidentiality, clause, information, concern...  \n",
       "4  [information, concerning, candidate, furnished...  \n",
       "5  [confidentiality, clause, information, concern...  \n",
       "6  [position, applied, new, business, sales, cros...  \n",
       "7  [pdf15, one, zero, obj, typecatalogpages, two,...  \n",
       "8  [confidentiality, clause, information, concern...  \n",
       "9  [confidentiality, clause, information, concern...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get most occuring words top ten words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hr 31\n",
      "management 26\n",
      "business 16\n",
      "ensure 14\n",
      "development 12\n",
      "including 12\n",
      "training 12\n",
      "employees 11\n",
      "manager 11\n",
      "line 10\n"
     ]
    }
   ],
   "source": [
    "# for x in data['resume']:\n",
    "#     count = Counter(x)\n",
    "#     arranged = sorted(count.iteritems(), key=lambda (k, v): (-v, k))[:10]\n",
    "#     for name, score in arranged:\n",
    "#         print name, score\n",
    "#     print (\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1249, 20)\n",
      "[[ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " ...\n",
      " [-1.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
